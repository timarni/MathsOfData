{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we consider a binary classification task that we will model using logistic regression. Your goal will be to find a classifier using first-order methods and accelerated gradient descent methods. The first part will consist of more theoretical questions, and the second one will ask you to implement these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  ℹ️ <strong>Information on group based work:</strong>\n",
    "</div>\n",
    "\n",
    "- You are to deliver only 1 notebook per group.\n",
    "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
    "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
    "  ⚠️ Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person 1 **Name**: Tim Arni || Person 1 **SCIPER**: 274586\n",
    "\n",
    "\n",
    "Person 2 **Name**: Adriana Orellana || Person 2 **SCIPER**: 376792\n",
    "\n",
    "\n",
    "Person 3 **Name**: Angel Zenteno || Person 3 **SCIPER**: 376890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression - 15 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classic approach to _binary classification_. Before we dive in, let us first define the standard logistic function $\\sigma$ on which most of what follows is built:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sigma : x \\mapsto \\frac{1}{1 + \\exp{(-x)}}.\n",
    "\\end{equation*}\n",
    "\n",
    "In logistic regression, we model the _conditional probability_ of observing a class label $b$ given a set of features $\\mathbf{a}$. More formally, if we observe $n$ independent samples\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{a}_i\\in\\mathbb{R}^p$ and $b_i\\in\\{-1, +1\\}$ is the class label, we _assume_ that $b_i$ given $\\mathbf{a}_i$ is a symmetric Bernouilli random variable with parameter $\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)$, for some unknown $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$. In other words, we assume that there exists an $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = 1 \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural) \\quad \\text{ and } \\quad \\mathbb{P}(b_i = -1 \\mid \\mathbf{a}_i) = 1 - \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)=  \\sigma( - \\mathbf{a}_i^T\\mathbf{x}^\\natural).\n",
    "\\end{equation*}\n",
    "\n",
    "This is our statistical model. It can be written in a more compact form as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma(j \\cdot \\mathbf{a}_i^T\\mathbf{x}^\\natural), \\quad j \\in \\{+1, -1\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal now is to determine the unknown $\\mathbf{x}^\\natural$ by constructing an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ (1 point) We are provided with a set of $n$ independent observations. Show that the negative log-likelihood $f$ can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tf(\\mathbf{x}) = -\\log(\\mathbb{P}(b_1, \\dots, b_n | a_1, \\dots, a_n)) & = \\sum_{i=1}^n  \\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x})).\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Answer to a)__\n",
    "\n",
    "$\n",
    "f(x) = - \\log \\left( P(b_1, ..., b_n | a_1, ..., a_n) \\right) \\quad \\text{| using that the samples are i.i.d.}\n",
    "$\n",
    "\n",
    "$\n",
    "= - \\log \\left( \\prod_{i=1}^{n} P(b_i | a_i) \\right) \\quad \\text{| using} \\ P(b_i | a_i) = \\frac{1}{1 + \\exp(-b_i a_i^T x)}\n",
    "$\n",
    "\n",
    "$\n",
    "= - \\log \\left( \\prod_{i=1}^{n} \\frac{1}{1 + \\exp(-b_i a_i^T x)} \\right) \\quad \\text{| using} \\ \\log(a \\cdot b) = \\log(a) + \\log(b)\n",
    "$\n",
    "\n",
    "$\n",
    "= - \\sum_{i=1}^{n} \\log \\left( \\frac{1}{1 + \\exp(-b_i a_i^T x)} \\right) \\quad \\text{| using} \\ -\\ln(a) = \\ln \\left( \\frac{1}{a} \\right)\n",
    "$\n",
    "\n",
    "$\n",
    "= \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-b_i a_i^T x) \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 point) Show that the function $u \\mapsto \\log(1 + \\exp(-u))$ is convex. Deduce that $f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Answer to b)__ \n",
    "\n",
    "First, using the fact that if the second derivative of a function $g''(u)$ is $\\geq 0$ $\\forall u \\in I$, then $g$ is convex on $I$, we show that $u \\mapsto \\log(1 + \\exp(-u))$ is convex on $\\R$\n",
    "\n",
    "> First, we calculate $g'(u)$: <br>\n",
    "> $g'(u) = \\frac{1}{1 + \\exp(-u)} \\cdot (-\\exp(-u))$ <br>\n",
    "> Now, we calculate $g''(u)$: <br>\n",
    "> $g''(u) = \\frac{\\exp(-u) \\cdot (1 + \\exp(-u)) - (\\exp(-u)) \\cdot \\exp(-u)}{(1 + \\exp(-u))^2}$ <br>\n",
    "> Simplifying the numerator: <br> \n",
    "> $g''(u) = \\frac{\\exp(-u)}{(1 + \\exp(-u))^2}$ <br>\n",
    "> Since the numerator and denumerator are both non-negative $\\forall u \\in \\R$, $g''(u) \\geq 0$ $\\forall u \\in \\R$, which shows that the function is convex.\n",
    "\n",
    "\n",
    "\n",
    "Second, show that $f(x) = \\sum_{i=1}^{n} \\log(1 + \\exp(-b_i a_i^T x))$ is convex:\n",
    "\n",
    "> To do so, we use the following facts:\n",
    "> * $\\log(1 + \\exp(-b_i a_i^T x))$ is convex, because the composition of a convex and a linear function is convex. Here the linear function is $x \\mapsto -b_i a_i^T x$ and the convex function is $u \\mapsto \\log(1 + \\exp(-u))$, which is convex as shown above.\n",
    "> * The sum of convex functions is convex. <br>\n",
    "\n",
    "As shown, $f(x)$ is the sum of convex functions and therefore also convex\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator, which is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^\\star_{ML} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}),\n",
    "\\end{equation*}\n",
    "\n",
    "is a global minimum so it can serve as an estimator of $\\mathbf{x}^\\natural$. But, does the minimum always exist? We will ponder this question in the following three points.\n",
    "\n",
    "__(c)__ (1 point) Explain the difference between infima and minima.  Give an example of a convex function, defined over $\\mathbb{R}$, that does not attain its infimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer to (c)__\n",
    "\n",
    "The *minima* refers to the smallest value in a given set, so the minima has to be part of the given set. The *infima* is the greatest lower bound to a set and does not necessarily have to be an element of the set. The function $f(x) = e^x$ has 0 as infimum, but does never attain it, because $\\pm \\infty \\notin \\mathbb{R}$. $f(x) = e^x$ is convex, because $f''(x) = e^x$ is $\\geq 0$ $\\forall x \\in \\R$ (to be precise, $f(x) = e^x$ is even strictly convex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (1 point) Let us assume that there exists $\\mathbf{x}_0 \\in \\mathbb{R}^p$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0.\n",
    "\\end{equation*}\n",
    "\n",
    "This is called _complete separation_ in the literature. Can you think of a geometric reason why this name is appropriate? Think of a 2D example where this can happen (i.e $p=2$) and describe why _complete separation_ is an appropriate name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer to (d)__\n",
    "\n",
    "*Complete separation* means that there exists a hyperplane (characterized by $x_0$), which perfectly separates the data, such that all the datapoints $a_i$ whose corresponding $b_i = -1$ are on one side of the hyperplane and all the $a_i$ whose corresponding $b_i = 1$ are on the other side of the hyperplane. Therefore the name *complete separation* is appropriate.\n",
    "\n",
    "In 2D ($p = 2$) the separating hyperplane is a line (characterized by its intercept $x_{0,1}$ and slope $x_{0, 2}$). If $b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0 \\forall i\\in\\{1, \\dots, n\\}$ implies that it exists a $x_0$ such that for all points $a_i$ the label $b_i$ determines on which side of the line determined by $x_0$ the points lie. Such a separating hyperplane would be a perfect solution for our training data set, since every point of the training set is labeled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you should see that it is likely that our data satisfies the complete separation assumption. Unfortunately, as you will show in the following question, this can become an obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (1 point) In a _complete separation_ setting, i.e, there exists $\\mathbf{x}_0$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0,\n",
    "\\end{equation*}\n",
    "\n",
    "prove that the function $f$ does not attain its minimum. \n",
    "\n",
    "__Hint__: If the function did have a minimum, would it be above, below or equal to zero? Then think of how $f(2 \\mathbf{x}_0)$ compares with $f(\\mathbf{x}_0)$, how about $f(\\alpha \\mathbf{x}_0)$ for $\\alpha \\rightarrow + \\infty$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer to (e)__\n",
    "\n",
    "_Observations_\n",
    "* First, we observe that $f(x) = \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-b_i a_i^T x) \\right) \\geq 0 \\: \\forall b_i, \\: a_i \\: and \\: x $.\n",
    "\n",
    "* Secondly, we observe that in a complete separation setting $f(x) \\rightarrow 0$ when $x \\rightarrow + \\infty$.\n",
    "\n",
    "_Compare_ $f(x_0)$ and $f(\\alpha x_0)$ <br>\n",
    "\n",
    "* Scaling $f(x_0)$ by $\\alpha \\geq 0$, the new function value is: $ f(\\alpha x_0) = \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(- \\alpha (b_i a_i^T x)) \\right)$\n",
    "\n",
    "* Since $b_i \\mathbf{a}_i^T \\mathbf{x}_0 > 0$, increasing $\\alpha$ will make $\\alpha b_i \\mathbf{a}_i^T \\mathbf{x}_0$ larger. As $\\alpha \\to +\\infty$, $\\log \\left( 1 + \\exp(- \\alpha (b_i a_i^T x)) \\right) \\to 0 \\: \\forall i$\n",
    "\n",
    "* Therefore, for large $\\alpha$, the overall function $f(\\alpha \\mathbf{x}_0)$ will approach 0.\n",
    "\n",
    "_Conclusion_\n",
    "\n",
    "Since $f(\\alpha \\mathbf{x}_0) \\to 0$ as $\\alpha \\to +\\infty$, the function can get arbitrarily close to 0 but will never reach 0. Therefore, the function does not attain its minimum in the case of complete separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just shown convex functions do not always attain their infimum. So it is possible for the maximum-likelihood estimator $\\mathbf{x}^\\star_{ML}$ to not exist. We will resolve this issue by adding a regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we consider the function\n",
    "\n",
    "\\begin{equation*}\n",
    "\tf_\\mu(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2\n",
    "\\end{equation*}\n",
    "with $\\mu> 0$.\n",
    "\n",
    "__(f)__ (1 point) Show that the gradient of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "__Hint__: Lecture 3 shows you how to proceed with this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer to (f)__\n",
    "\n",
    "As seen in class (lecture 3, slide 21) the gradient of $ f(x) = \\log \\left( 1 + \\exp(-b a^T x) \\right)$ can be calculated using the chain rule via Jacobians, by considering $f(x) = g(x) \\circ h(x) = g(h(x))$ with:\n",
    "* $g(x) = \\log \\left( 1 + \\exp(-bx) \\right)$, whose Jacobian is $J_g(x) = -b \\frac{exp(-bx)}{1 + exp(-bu)}$\n",
    "* $h(x) = \\mathbf{a^Tx}$, whose Jacobian is $J_h(x) = \\mathbf{a^T}$\n",
    "\n",
    "By the chain rule, we get that $J_f(x) = J_g(h(x)) \\cdot J_h(x) = -b \\frac{exp(-b\\mathbf{a^Tx})}{1 + exp(-b(\\mathbf{a^Tx}))} \\mathbf{a^T}$\n",
    "\n",
    "And since the gradient is the transpose of the Jacobian, we get that the gradient of $ f(x) = \\log \\left( 1 + \\exp(-b_i a_i^T x) \\right)$ is $\\nabla f(x) = -b \\frac{exp(-b\\mathbf{a^Tx})}{1 + exp(-b(\\mathbf{a^Tx}))} \\mathbf{a}$ which is equal to $\\nabla f(x) = -b \\sigma(-b(\\mathbf{a^Tx})) \\mathbf{a}$ using the definition of the sigmoid function.\n",
    "\n",
    "Combining the results found above with the observations that:\n",
    "* taking the gradient is a linear operation, so the gradient of a sum is equal to the sum of the individual gradients\n",
    "* the gradient of $\\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2$ is $\\mu \\mathbf{x}$ (trivial, see lecture 3 - slide 17)\n",
    "\n",
    "We prove that $\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x}))\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "\\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer to(g)__\n",
    "\n",
    "By observing the Gradient and the Hessian of a general function $f(\\mathbf{x})$:\n",
    "\n",
    "$$\n",
    "\n",
    "    \\nabla f(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial f}{\\partial x_p}\n",
    "    \\end{bmatrix}, \\quad\n",
    "    \\nabla^2 f(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_p} \\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_p \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_p \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_p^2}\n",
    "    \\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "we observe, that each column i of the Hessian is the partial derivative $\\frac{\\partial \\nabla f}{\\partial x_i}$ of the gradient.\n",
    "\n",
    "Furthermore, we note the following facts about the sigmoid function $\\sigma (x) = \\frac{1}{1+exp(-x)}$:\n",
    "* $1 - \\sigma(x) = \\frac{1 + exp(-x)}{1 + exp(-x)} - \\frac{1}{1 + exp(-x)} = \\frac{exp(-x)}{1 + exp(-x)}$\n",
    "* $\\sigma ' (x) = \\frac{exp(-x)}{(1 + exp(-x))^2} = \\sigma (x) (1-\\sigma(x))$\n",
    "\n",
    "Looking at the coordinate wise partial derivatives of $\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}$ we observe that:\n",
    "\n",
    "* The entries of the gradient are $\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^n -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})a_{ik} + \\mu x_k$\n",
    "\n",
    "Using the following facts, we can calculate the entries $\\frac{\\partial ^2 f}{\\partial x_k \\partial x_j}$ of the Hessian matrix:\n",
    "* the derivative of a sum is the sum of its derivatives\n",
    "* $\\sigma(h(u)) = \\sigma (h(u)) (1-\\sigma(h(u))) \\cdot h'(u)$\n",
    "* $\\frac{\\partial (-b_i\\mathbf{a_i^Tx})}{\\partial x_k} = \\frac{\\partial (-b_i(a_{i1}x_1+ \\dots + a_{ik}x_k + \\dots + a_{ip}x_p))}{\\partial x_k} = -b_i a_{ik}$\n",
    "* $\\frac{\\partial (\\mu x_k)}{\\partial x_j} = \\mu$ if $k = j$ and $0$ otherwise\n",
    "* $-b_i \\cdot -b_i = 1 \\: \\forall i$\n",
    "\n",
    "Which leads to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ^2 f}{\\partial x_k \\partial x_j} =\n",
    "\n",
    "\\begin{cases} \n",
    "\t\\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) a_{ik} a_{ij} + \\mu x_k & \\text{if } k = j \\\\\n",
    "\\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) a_{ik} a_{ij} & \\text{if } k \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Finally, writing this in vector form gives us:\n",
    "$$\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x}))\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to observe that we can write the Hessian in a more compact form by defining the matrix\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{a}_1^T & \\rightarrow \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_2^T & \\rightarrow \\\\\n",
    "         &  \\ldots &  \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_n^T & \\rightarrow \\\\\n",
    "  \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "It is easy to see that we have\n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) =  \\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)\\mathbf{A}+ \\mu \\mathbf{I}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer to (h)__\n",
    "\n",
    "#### Lemma\n",
    "The Lemma on page 38 of Lecture 3 tells us that a twice differentiable convexe function $f$ is $\\mu$-strongly convexe if and only if \n",
    "$$\n",
    "\\nabla ^2 f(\\mathbf{x}) \\succeq \\mu \\mathbf{I}, \\, \\forall \\mathbf{x} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "In (b) we have shown that $f$ is convex and in (f) & (g) that it is twice differentiable, so this Lemma applies to our function $f$. To prove that $\\nabla ^2 f(\\mathbf{x}) \\succeq \\mu \\mathbf{I}, \\, \\forall \\mathbf{x} \\in \\mathbb{R}$ we need to calculate all the Eigenvalues of the Hessian of $f$ and show that $\\mu$ is a lower bound to all the Eigenvalues.\n",
    "\n",
    "#### Analyze structure of Hessian\n",
    "The Hessian is composed of two terms:\n",
    "- $ \\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x}))\\right) \\mathbf{A} $, where the diagonal matrix `Diag` contains the values $\\sigma ' (x) = \\sigma(x)(1 - \\sigma(x)) \\in [0, 0.25]$\n",
    "- $\\mu \\mathbf{I}$, where $\\mu > 0$ and $\\mathbf{I}$ is the identity matrix.\n",
    "\n",
    "\n",
    "To simplify the notation, we will define the diagonal matrix $\\mathbf{D}(\\mathbf{x}) = \\text{Diag}(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})))$, so the Hessian becomes:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f_\\mu(\\mathbf{x}) = \\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A} + \\mu \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "#### Eigenvalue Calculations:\n",
    "\n",
    "##### (a) Eigenvalues of $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A}$:\n",
    "\n",
    "Since, $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A} = \\left( \\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A} \\right)^T$, is symmetric and the fact that all entries in $\\mathbf{A}$ and $\\mathbf{D}$ are real ensures that all eigenvalues of $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A}$ are real.\n",
    " \n",
    "As shown, the entries of the diagonal matrix are between 0 and 0.25, which means that $\\mathbf{D}(\\mathbf{x})$ is a positive semi-definite matrix.\n",
    "\n",
    "Furthermore, the matrix $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A}$ can be viewed as a quadratic form:\n",
    "$$\n",
    "\\mathbf{v}^T \\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A} \\mathbf{v} = (\\mathbf{A} \\mathbf{v})^T \\mathbf{D}(\\mathbf{x}) (\\mathbf{A} \\mathbf{v}),\n",
    "$$\n",
    "and since $\\mathbf{D}(\\mathbf{x})$ is positive semi-definite, it follows that $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A}$ is also positive semi-definite. Thus, all eigenvalues $\\lambda_i$ of $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A}$ are non-negative and real\n",
    "\n",
    "##### (b) Eigenvalues of $\\mu \\mathbf{I}$:\n",
    "Since the  identity matrix has eigenvalue 1 for all dimensions, all the eigenvalues of $\\mu \\mathbf{I}$ are $\\mu$.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "We have shown that $\\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A} \\succeq 0$ and that the eigenvalues of $\\mu \\mathbf{I}$ are $\\mu$ therefore:\n",
    "$$\n",
    "\\nabla^2 f_\\mu(\\mathbf{x}) = \\mathbf{A}^T \\mathbf{D}(\\mathbf{x}) \\mathbf{A} + \\mu \\mathbf{I} \\succeq \\mu \\mathbf{I} \\Rightarrow f_{\\mu} \\; \\text{is $\\mu$ -strongly convex}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i)__ (3 points) Is it possible for a strongly convex function to not attain its minimum ? <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Justify your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, a $\\mu$-strongly convex function will always attain its minimum. This is because $\\mu$-strong convexity guarantees that $f$ has a certain curvature. The $\\mu$-strongly convex function $f(\\mathbf{x})$ will always lie above the quadratic lower bound given by $\\mu \\|\\mathbf{x}\\|_2 ^2$. Since $f(\\mathbf{x})$ is at least as curved as $\\mu \\|\\mathbf{x}\\|_2 ^2$, it will therefore reach its unique minimum, as it is bounded below by the upward-bending curves of $\\mu \\|\\mathbf{x}\\|_2 ^2$.\n",
    "\n",
    "As seen in lecture 3, slide 37, a function $f$ is $\\mu$-strongly convex if\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = h(\\mathbf{x})  + \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2_2\n",
    "$$\n",
    "\n",
    "where $h(\\mathbf{x})$ is a convex function.\n",
    "\n",
    "As shown in (c), there are convex functions that converge to their infima when $\\mathbf{x} \\rightarrow \\pm \\infty$ but never attain it. Looking at the definition of strong convexity stated above, we note that if $h(\\mathbf{x})$ has an infimum that it converges to when $\\mathbf{x} \\rightarrow \\pm \\infty$, then the term $\\frac{\\mu}{2}\\|\\mathbf{x}\\|^2_2$ goes towards $\\infty$ when $\\mathbf{x} \\rightarrow \\pm \\infty$, and therefore $f(\\mathbf{x})$ also goes to $\\infty$. Therefore, we can conclude that $f(\\mathbf{x})$ cannot approach an infimum and instead reaches its minimum $\\mathbf{x}^\\star$ when $\\nabla f(\\mathbf{x}^\\star) = \\nabla h(\\mathbf{x}^\\star)  + \\mu\\mathbf{x}^\\star = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that $f_\\mu$ is smooth, i.e, $\\nabla f_\\mu$ is L-Lipschitz with respect to the Euclidean norm, with \n",
    "\\begin{equation}\n",
    "\tL = \\|A\\|^2_F + \\mu \\text{, where }\\|\\cdot\\|_F\\text{ denotes the Frobenius norm. }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point for all three questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer (j-1)__\n",
    "\n",
    "By the form of the matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{a_i}\\mathbf{a_i^T} =\n",
    "\\begin{bmatrix}\n",
    "    a_1^2 & a_1 a_2 & \\cdots & a_1 a_p\\\\\n",
    "    a_1 a_2 & a_2^2 & \\cdots & \\vdots \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_1 a_p & \\cdots & \\cdots & a_p^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "it is easy to see that the rank of $\\mathbf{a_i}\\mathbf{a_i^T}$ is equal to 1, since all the columns are a scaled version of $\\mathbf{a_i}$. Furthermore, we know that all the eigenvalues are real, since $\\mathbf{a_i}\\mathbf{a_i^T}$ is symetric and that the number of non-zero eigenvalues is equal to the rank.\n",
    "\n",
    "Since the rank is 1, we try to find the one Eigenvalue of $\\mathbf{a_i}\\mathbf{a_i^T}$:\n",
    "\n",
    "We note, that the following equation holds for any $x \\in \\mathbb{R}^p$\n",
    "$$\n",
    "(\\mathbf{a_i}\\mathbf{a_i^T}) \\mathbf{x} = (\\mathbf{a_i} \\cdot \\mathbf{x}) \\mathbf{a_i}\n",
    "$$\n",
    "\n",
    "This equation holds for $\\mathbf{x} = \\mathbf{a_i}$, which leads to\n",
    "$$\n",
    "(\\mathbf{a_i}\\mathbf{a_i^T}) \\mathbf{a_i} = (\\mathbf{a_i} \\cdot \\mathbf{a_i}) \\mathbf{a_i} = ||\\mathbf{a_i}||^2 \\mathbf{a_i}\n",
    "$$\n",
    "\n",
    "so the Eigenvalue associated to the Eigenvector $\\mathbf{a_i}$ is $||\\mathbf{a_i}||^2$ and since $\\mathbf{a_i}\\mathbf{a_i^T}$ has rank 1 and therefore only one non-zero Eigenvalue, we conclude that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-2)__ Using [2](#mjx-eqn-eq2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$. \n",
    "\n",
    "__Hint__: Recall that $\\lambda_{\\max}(\\cdot)$ verifies the triangle inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer (j-2)__\n",
    "\n",
    "Using that $\\lambda_{\\text{max}}(\\cdot)$ verifies the triangular inequality and the expression we found for $\\nabla^2 f_{\\mu}(x)$ in (g), we have:\n",
    "\n",
    "$$\n",
    "\\lambda_{\\text{max}} \\left( A^T D(x) A + \\mu I \\right) \\leq \\lambda_{\\text{max}} \\left( A^T D(x) A \\right) + \\lambda_{\\text{max}}(\\mu I)\n",
    "$$\n",
    "\n",
    "with $x = \\sigma (b_i a_i x) (1 - \\sigma (b_i a_i x)) \\in [0, 1]$.\n",
    "\n",
    "First, we look at $\\lambda_{\\text{max}} (\\mu I)$. It is easy to see that for $\\mu I$ all eigenvalues are $\\mu$, so $\\lambda_{\\text{max}} (\\mu I) = \\mu$.\n",
    "\n",
    "Second, we notice that\n",
    "$$\n",
    "\\lambda_{\\text{max}} \\left( A^T D(x) A \\right) \\leq \\lambda_{\\text{max}} \\left( A^T A \\right)\n",
    "$$\n",
    "because the entries in the diagonal matrix $D(x)$ are in $[0, 1]$, and will therefore just scale the eigenvalues of $A^T A$ down. So,\n",
    "$$\n",
    "D(x) \\preceq I \\Rightarrow A^T D(x)A \\preceq A^T A \\Rightarrow \\lambda _{max}(A^T D(x)A) \\leq \\lambda _{max}(A^T A)\n",
    "$$\n",
    "\n",
    "Now we need to get $\\lambda _{max}(A^T A)$:\n",
    "\n",
    "Using the fact that $$ A^T A = \\sum_{i=1}^n a_i a_i^T $$ and the relationship proven in (j-1), we have that:\n",
    "$$\n",
    "\\lambda_{\\text{max}} (A^T A) = \\lambda_{\\text{max}} \\left( \\sum_{i=1}^n a_i a_i^T \\right) \\leq \\sum_{i=1}^n \\lambda_{\\text{max}}(a_i a_i^T) = \\sum_{i=1}^n \\|a_i\\|^2\n",
    "$$\n",
    "by using (j-1).\n",
    "\n",
    "Putting this together, we get that\n",
    "$$\n",
    "\\lambda_{\\text{max}} \\left( \\nabla^2 f_{\\mu}(x) \\right) = \\lambda_{\\text{max}} \\left( A^T D(x) A + \\mu I \\right) \\leq \\lambda_{\\text{max}} \\left( A^T A + \\mu I \\right) = \\sum_{i=1}^n \\|a_i\\|^2 + \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|A\\|_F^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer (j-3)__\n",
    "\n",
    "Using the definition of $L$-smoothness (lecture 3, slide 39), we have that $f$ is $L$-smooth if $f$ is twice differentiable and $\\nabla^2 f(x) \\preceq L \\mathbf{I}$.\n",
    "\n",
    "We have shown in (f) and (g) that $f$ is twice differentiable and in (j-2) that $\\lambda_{\\text{max}} \\left( \\nabla^2 f_{\\mu}(x) \\right) \\leq \\sum_{i=1}^n \\|a_i\\|^2 + \\mu $.\n",
    "\n",
    "For a matrix $A$ with real values entries we have that $\\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 = \\|A\\|_F^2$ and by setting $L = \\|A\\|_F^2 + \\mu$ we can guarantee that $\\nabla^2 f(x) \\preceq L \\mathbf{I}$, since we have shown in (j - 2) that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq L$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(l)__ (2 point) To finalize, we introduce the Kullback-Leibler (KL) divergence. The KL divergence is a measure of how one probability distribution differs from a second, reference probability distribution. Along side the examples we saw in slide 18 of Lecture 1, the KL divergence is also a useful loss function to be used in learning frameworks.\n",
    "\n",
    "Write the definition of the Kullback-Leibler (KL) divergence between the true label distribution $q(b_i∣\\mathbf{a}_i)$ and the model’s predicted distribution $p(b_i∣\\mathbf{a}_i)$ and show that minimizing the KL divergence between $q(b_i∣\\mathbf{a}_i)$ and $p(b_i∣\\mathbf{a}_i)$ is equivalent to minimizing the negative log-likelihood derived in (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer (l)__\n",
    "\n",
    "#### 1. Definition of the Kullback-Leibner divergence\n",
    "\n",
    "Using the definiton of the KL-Divergence found here (https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):\n",
    "\n",
    "   $$\n",
    "   D_{\\text{KL}}(P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right)\n",
    "   $$\n",
    "\n",
    "The KL-Divergence can be seen as a statistical distance that measures how the probability distribution P is different from the probability distribution Q.\n",
    "\n",
    "For our problem, we formulate the KL-Divergence the following way:\n",
    "\n",
    "   $$\n",
    "   D_{\\text{KL}}(q \\parallel p) = \\sum_{i=1}^n q(b_i \\mid \\mathbf{a}_i) \\log \\left( \\frac{q(b_i \\mid \\mathbf{a}_i)}{p(b_i \\mid \\mathbf{a}_i)} \\right)\n",
    "   $$\n",
    "\n",
    "where\n",
    "- $q(b_i \\mid \\mathbf{a}_i)$: true label distribution\n",
    "- $p(b_i \\mid \\mathbf{a}_i)$: predicted distribution\n",
    "\n",
    "#### 2. Show that minimizing KL-Divergence = minimizing negative log-likelihood\n",
    "\n",
    "We can split up the KL-divergence definition into two sums and realize that the first term is a constant (because it only depends on q - the distribution of the data), so when minimizig the KL-divergence we can only focus on the second term:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q \\parallel p) = \\sum_{i=1}^n q(b_i \\mid \\mathbf{a}_i) \\log \\left( q(b_i \\mid \\mathbf{a}_i) \\right) - \\sum_{i=1}^n q(b_i \\mid \\mathbf{a}_i) \\log \\left( p(b_i \\mid \\mathbf{a}_i) \\right) = \\text{constant} -\\sum_{b_i} q(b_i \\mid \\mathbf{a}_i) \\log \\left( p(b_i \\mid \\mathbf{a}_i) \\right)\n",
    "$$\n",
    "\n",
    "$q(b_i \\mid \\mathbf{a}_i)$ can be seen as an indicator variable for the true class, so we are only considering the predicted log probability of the correct class. So in the case of minimizing the KL-Divergence we have (by following the same computational steps as in (a)):\n",
    "\n",
    "$$\n",
    "\\arg\\min_{p} D_{KL}(q \\parallel p) = \\arg\\min_{p} -\\sum_{i=1}^n  \\log p(b_i \\mid \\mathbf{a}_i) = \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\mathbf{x}} - \\sum_{i=1}^{n} \\log \\left( \\frac{1}{1 + \\exp(-b_i a_i^T x)} \\right) = \\arg\\min_{\\mathbf{x}} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-b_i a_i^T x) \\right) = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Where $f(x)$ is equal to the negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From your work in this section, you have shown that the maximum likelihood estimator for logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_2^2$ regularizer. Consequently, the estimator for $\\mathbf{x}^\\natural$ we will use will be the solution of the smooth strongly convex problem,\n",
    "\\begin{equation}\n",
    "\t\\mathbf{x}^\\star=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2.\n",
    "\\tag{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) TA's will give you candy if you provide a complete proof."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
