{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcEtGJ8QeYP"
   },
   "source": [
    "# 3. Sharpness-aware minimization (SAM) with Sparse Networks - 25 points\n",
    "\n",
    "## 3.1 Get a sparse networks through pruning\n",
    "**Pruning** is a technique used to reduce the size and complexity of a neural network model by removing (setting to zero) less important parameters. The goal is to create a more efficient model that retains its predictive accuracy while being smaller, which can improve both inference speed and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWbesreILh4"
   },
   "source": [
    "Let's train a simple model on the MNIST dataset to learn about pruning at first. We just use 10% of the dataset for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJta4OIBcEpy"
   },
   "source": [
    "### 3.1.1 Train a dense network with SGD\n",
    "Let us first train a dense model with SGD. We reuse the model for the discriminator of the GAN in Homework 2 and name it 'Classifier'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXSZdsCILh4"
   },
   "outputs": [],
   "source": [
    "from lib.part3.utils import *\n",
    "max_epochs = 10\n",
    "device = \"cpu\" # Change this if you can and want to use a GPU device\n",
    "model = Classifier().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLI-Alb-ILh5"
   },
   "source": [
    "We define the optimizing process of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXZG0iPPILh5"
   },
   "outputs": [],
   "source": [
    "def optimize_sgd(model, optimizer, img, label):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udzDwcQqX-GE"
   },
   "source": [
    "The following cell runs the training loop, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y_p6FbG0ILh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.931 accuracy on the validation set.\n",
      "Epoch 1 with 0.946 accuracy on the validation set.\n",
      "Epoch 2 with 0.954 accuracy on the validation set.\n",
      "Epoch 3 with 0.957 accuracy on the validation set.\n",
      "Epoch 4 with 0.963 accuracy on the validation set.\n",
      "Epoch 5 with 0.966 accuracy on the validation set.\n",
      "Epoch 6 with 0.967 accuracy on the validation set.\n",
      "Epoch 7 with 0.971 accuracy on the validation set.\n",
      "Epoch 8 with 0.971 accuracy on the validation set.\n",
      "Epoch 9 with 0.973 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, optimize_sgd, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnvFT-DxILh6"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKTkbxGyILh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9771 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgVCA_jwJbPx"
   },
   "source": [
    "### 3.1.2 Sparse network with magnitude-based pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubynCgs2b0wT"
   },
   "source": [
    "Magnitude-based pruning specifically focuses on **removing weights that have the smallest absolute values**, under the assumption that weights with smaller magnitudes contribute less to the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkjQPvAXcZcI"
   },
   "source": [
    "**(1)** (6 points) Realize magnitude-based pruning below, which removes a part of weights that have the smallest absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xfb19GShJZFI"
   },
   "outputs": [],
   "source": [
    "def magnitude_prune(model, prune_fraction):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            # FILL: Get weight's absolute values\n",
    "            weight_abs = abs(param.data)\n",
    "            # FILL: Compute the threshold\n",
    "            threshold = torch.quantile(weight_abs.flatten(), prune_fraction)\n",
    "            # FILL: Prune weights below the threshold\n",
    "            weight_abs[weight_abs < threshold] = 0\n",
    "            weight_mask = (weight_abs != 0).float()\n",
    "            param.data *= weight_mask\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fr0VzGC_NPtv"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# Copy a model for pruning\n",
    "sparse_model = copy.deepcopy(model)\n",
    "# Get a sparse model by pruning 50% parameters\n",
    "sparse_model = magnitude_prune(sparse_model, prune_fraction=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sOOCdBoRgzL"
   },
   "source": [
    "Copy a sparse model for SAM implementation later in 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xzi_XqlmRbC4"
   },
   "outputs": [],
   "source": [
    "sparse_model_sam = copy.deepcopy(sparse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_EFs4FiQh1X"
   },
   "source": [
    "Evaluation after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4ZtXqwuaPtNF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.948 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz5gPbMuGiAW"
   },
   "source": [
    "### 3.1.3 Finetune the sparse model\n",
    "\n",
    "We finetune the sparse model after pruning with SGD to recover its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jD0tZKllNi00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.942 accuracy on the validation set.\n",
      "Epoch 1 with 0.942 accuracy on the validation set.\n",
      "Epoch 2 with 0.942 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "finetune_epoch = 3\n",
    "train_model(sparse_model, optimizer, optimize_sgd, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rha2SQXBQnV8"
   },
   "source": [
    "Evaluate the sparse model after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sby9YSR9PrYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9544 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHZY8_nZaMO-"
   },
   "source": [
    "**(2)** (2 point) What are the pros and cons of sparse networks?\n",
    "\n",
    "<font color=‘blue’>\n",
    "\n",
    "**Answer:** \n",
    "The pros of sparse networks are that they reduce the number of parameters by setting them to zero, which leads to lower memory usage and faster inference speed. Pruning also helps models to learn more robust features, which improves generalization and reduces overfitting. However, the prune fraction (used to determine the threshold) must be carefully chosen to avoid losing important information for the model. \n",
    "\n",
    "The cons are that the accuracy or performance of these models might be worse than models that do not use pruning. Moreover, the training process can be harder to optimize and may show slower convergence. In our above implementation, after pruning the model, the accuracy dropped from 97% to 95%, and despite trying different numbers of epochs for finetuning, the accuracy got stuck during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97IxkV9Rl5bT"
   },
   "source": [
    "## 3.2 Train the sparse model with SAM\n",
    "\n",
    "Sharpness-aware minimization (SAM) is a new optimization technique, which is satisfied with not just a low loss, instead it seeks a neighborhood with uniformly low loss. SAM is motivated by the link between the geometry of the loss landscape and generalization. It makes sense that a low loss within a uniformly low loss neighborhood will generalize better than a low loss within a region of higher variance.\n",
    "\n",
    "To be specific, we consider a model with the weight vector of $\\mathbf{w}$ and the training loss $L_S$. SAM aims to minimize the maximum loss within a small region which is usually a $\\ell_2$ ball with $\\rho$ radius. Note that $\\rho$ is a small value close to $0$. Therefore, SAM can be formulated as a minimax optimization problem:\n",
    "$$\\min_{\\mathbf{w}} \\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2\\leq \\rho} L_S (\\mathbf{w} + \\mathbf{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFY_FOCEILh3"
   },
   "source": [
    "**(3)** (3 points) Please solve the inner maximum problem by first-order Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u8vP9FCtTuh"
   },
   "source": [
    "<font color=‘blue’>\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "First-order Taylor series (linear approximation):\n",
    "$$\n",
    "f(x) \\approx f(x_0) + f'(x_0)(x-x_0)\n",
    "$$\n",
    "\n",
    "Inner maximum problem: \n",
    "$$\n",
    "\\max_{\\varepsilon: \\|\\varepsilon\\|_2 \\leq \\rho} L_s(w + \\varepsilon) \\text{ where } \\rho \\approx 0\n",
    "$$\n",
    "\n",
    "1° Applying first-order series considering the following:\n",
    "$$\n",
    "f(x) = L_s(w+\\varepsilon), \\quad f'(x_0) = \\nabla_w L_s(w), \\quad f(x_0) = L_s(w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_s(w+\\varepsilon) \\approx L_s(w) + \\nabla_w L_s(w)^T(w+\\varepsilon-w)\n",
    "$$\n",
    "$$\n",
    "L_s(w+\\varepsilon) \\approx L_s(w) + \\nabla_w L_s(w)^T\\varepsilon\n",
    "$$\n",
    "\n",
    "2° Then the maximization problem can be written as:\n",
    "$$\n",
    "\\max_{\\varepsilon: \\|\\varepsilon\\|_2 \\leq \\rho} L_s(w) + \\nabla_w L_s(w)^T\\varepsilon\n",
    "$$\n",
    "\n",
    "3° We must only focus on $\\nabla_w L_s(w)^T\\varepsilon$ since it depends on $\\varepsilon$:\n",
    "$$\n",
    "\\max_{\\varepsilon: \\|\\varepsilon\\|_2 \\leq \\rho} \\nabla_w L_s(w)^T\\varepsilon\n",
    "$$\n",
    "\n",
    "4° Now, we must find $\\varepsilon^*$ that maximizes the dot product:\n",
    "$$\n",
    "\\nabla_w L_s(w)^T\\varepsilon^* = \\|\\nabla_w L_s(w)\\|_2 \\|\\varepsilon^*\\|_2 \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "5° When these two vectors points to the same direction $\\theta = 0$, we get the maximum value. Thus, the $\\varepsilon^*$ should be aligned in the direction of $\\nabla_w L_s(w)$:\n",
    "$$\n",
    "\\varepsilon^* = k\\nabla_w L_s(w)\n",
    "$$\n",
    "\n",
    "6° $k$ is a scalar multiple of $\\nabla_w L_s(w)$. However, we must be careful with the constraint: $\\varepsilon: \\|\\varepsilon\\|_2 \\leq \\rho$. Therefore we can get the unit vector of $\\nabla_w L_s(w)$ to ensure the direction is correct:\n",
    "$$\n",
    "\\varepsilon^* = k\\frac{\\nabla_w L_s(w)}{\\|\\nabla_w L_s(w)\\|_2}\n",
    "$$\n",
    "\n",
    "7° Finally, $k$ can take the maximum value of $\\|\\varepsilon\\|_2$ that is $\\rho$, because multiplying the unit vector with $\\rho$ satisfies the constraint, which gives us the optimal perturbation $\\varepsilon^*$:\n",
    "$$\n",
    "\\varepsilon^* = \\rho\\frac{\\nabla_w L_s(w)}{\\|\\nabla_w L_s(w)\\|_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwbpa28BILh6"
   },
   "source": [
    "**(4)** (8 points) Now we will train the same model using the SAM optimizer.\n",
    "Please implement SAM by the two steps below. The first step is for the maximizer which calculates $\\epsilon$ obtained in question (1). The second step is the normal step for the minimizer: $\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta_t \\nabla L_S (\\mathbf{w}_t + \\mathbf{\\epsilon}_t)$ where $\\eta_t$ is step size. Note that we set $\\rho=0.05$.\n",
    "\n",
    "Hint: be careful about weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7mZeWh05ILh6"
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, lr=0.01, rho=0.05):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, lr)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # Note that p.grad gets the gradient; p.data gets the weight.\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        norm += 1e-12 # Avoid zero norm\n",
    "        return norm\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        # Add the perturbation on the weight.\n",
    "        # Hint: the norm of the gradient can be calculated by _grad_norm() function.\n",
    "        # Hint: self.param_groups to get access to the weight and gradient of each parameter\n",
    "        # FILL\n",
    "        grad_norm = self._grad_norm()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    epsilon = self.defaults['rho'] * p.grad / grad_norm\n",
    "                    p.data += epsilon\n",
    "                    self.state['perturbations'][p] = epsilon\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        # FILL\n",
    "        # Hint: Remember to change the parameters back\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.data -= self.state['perturbations'][p]\n",
    "        \n",
    "        self.state['perturbations'] = {}\n",
    "            \n",
    "        self.base_optimizer.step()\n",
    "\n",
    "        self.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F6EaFSWILh6"
   },
   "source": [
    "Define an optimizer of `SAM` for the model. We recommend using `SGD` as base optimizer with a learning rate of $0.05$ (which is same with SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f3D9IZGql74k"
   },
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.SGD\n",
    "sam_optimizer = SAM(sparse_model_sam.parameters(), base_optimizer, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYNhqGoILh6"
   },
   "source": [
    "**(5)** (4 points) Please define the optimizing process of SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q9-zRaSVCwGO"
   },
   "outputs": [],
   "source": [
    "def optimize_sam(model, optimizer, img, label):\n",
    "    enable_running_stats(model)\n",
    "    # First forward-backward pass\n",
    "    # FILL\n",
    "    # Hint: use sam_optimizer above\n",
    "    # Hint: use loss 'cross_entropy'\n",
    "    pred = model(img)\n",
    "    loss = cross_entropy(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.first_step()\n",
    "\n",
    "    disable_running_stats(model)\n",
    "    # Second forward-backward pass\n",
    "    # FILL\n",
    "    pred = model(img)\n",
    "    loss = cross_entropy(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "45KsL4m4DAU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.964 accuracy on the validation set.\n",
      "Epoch 1 with 0.966 accuracy on the validation set.\n",
      "Epoch 2 with 0.964 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(sparse_model_sam, sam_optimizer, optimize_sam, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSh_GOxoNE-"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the sparse model finetuned with SAM on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IAfT8aMoNWb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9789 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model_sam)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi2NLImh_QeD"
   },
   "source": [
    "**(6)** (2 points) Give a conclusion comparing SAM with SGD. Is there any drawback of SAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "**Answer:**\n",
    "SAM improves generalization by seeking parameters that are in neighborhoods with uniformly low loss. In contrast, SGD only focuses on minimizing the loss considering the current parameters. As a result of applying both methods in the previous problem, we can see that SAM achieved a higher accuracy of 97.8% compared to the 95% obtained by SGD on the test set. However, SGD is clearly faster than SAM in terms of training time. SAM requires two forward-backward passes for each iteration, which makes it slower than SGD. Moreover, SAM needs to store the perturbations to change the parameters back in the second optimization step, which increases memory usage. Therefore, the main drawbacks of SAM are the computational cost and memory usage, which can be a problem for larger models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
